{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer-Based IDS Implementation\n",
        "\n",
        "### 1. Purpose\n",
        "\n",
        "This notebook implements a transformer-based Intrusion Detection System (IDS) using the preprocessed dataset produced in 01_dataset_preprocessing.ipynb. Unlike the autoencoder approach, the transformer is trained in a supervised manner on both benign and attack traffic, learning to classify network flows directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Load Processed Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (1979513, 77)\n",
            "Validation shape: (424181, 77)\n",
            "Test shape: (424182, 77)\n",
            "\n",
            "Label distribution (Train):\n",
            "0    1589924\n",
            "1     389589\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label distribution (Val):\n",
            "0    340698\n",
            "1     83483\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label distribution (Test):\n",
            "0    340698\n",
            "1     83484\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "X_train = pd.read_csv(\"processed/X_train.csv\").values\n",
        "X_val = pd.read_csv(\"processed/X_val.csv\").values\n",
        "X_test = pd.read_csv(\"processed/X_test.csv\").values\n",
        "\n",
        "y_train = pd.read_csv(\"processed/y_train.csv\").values.ravel()\n",
        "y_val = pd.read_csv(\"processed/y_val.csv\").values.ravel()\n",
        "y_test = pd.read_csv(\"processed/y_test.csv\").values.ravel()\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Validation shape:\", X_val.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n",
        "print(\"\\nLabel distribution (Train):\")\n",
        "print(pd.Series(y_train).value_counts().sort_index())\n",
        "print(\"\\nLabel distribution (Val):\")\n",
        "print(pd.Series(y_val).value_counts().sort_index())\n",
        "print(\"\\nLabel distribution (Test):\")\n",
        "print(pd.Series(y_test).value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Transformer Model Architecture\n",
        "\n",
        "The transformer model treats each feature as a token and uses self-attention to learn relationships between features. This allows the model to capture complex patterns in network traffic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Add positional encoding to input features\"\"\"\n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (seq_len, batch_size, d_model)\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerIDS(nn.Module):\n",
        "    \"\"\"Transformer-based Intrusion Detection System\"\"\"\n",
        "    def __init__(self, input_dim, d_model=128, nhead=8, num_layers=4, dim_feedforward=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Input projection to d_model\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len=input_dim)\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=False\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, input_dim)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Project input to d_model\n",
        "        x = self.input_projection(x)  # (batch_size, d_model)\n",
        "\n",
        "        # Reshape for transformer: (seq_len, batch_size, d_model)\n",
        "        # Treat each feature as a token\n",
        "        x = x.unsqueeze(0)  # (1, batch_size, d_model)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Apply transformer encoder\n",
        "        x = self.transformer_encoder(x)  # (1, batch_size, d_model)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = x.mean(dim=0)  # (batch_size, d_model)\n",
        "\n",
        "        # Classification\n",
        "        x = self.classifier(x)  # (batch_size, 1)\n",
        "\n",
        "        return x.squeeze(-1)  # (batch_size,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Convert to PyTorch Tensors and Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training batches: 3867\n",
            "Validation batches: 829\n",
            "Test batches: 829\n"
          ]
        }
      ],
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Handle class imbalance with weighted sampling\n",
        "class_counts = np.bincount(y_train.astype(int))\n",
        "class_weights = 1.0 / class_counts\n",
        "sample_weights = class_weights[y_train.astype(int)]\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "BATCH_SIZE = 512\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 550,273\n",
            "Trainable parameters: 550,273\n"
          ]
        }
      ],
      "source": [
        "input_dim = X_train.shape[1]\n",
        "model = TransformerIDS(\n",
        "    input_dim=input_dim,\n",
        "    d_model=128,\n",
        "    nhead=8,\n",
        "    num_layers=4,\n",
        "    dim_feedforward=256,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_x, batch_y in loader:\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = (outputs > 0.5).float()\n",
        "        correct += (preds == batch_y).sum().item()\n",
        "        total += batch_y.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = (outputs > 0.5).float()\n",
        "            correct += (preds == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "            all_preds.extend(outputs.cpu().numpy())\n",
        "            all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy, np.array(all_preds), np.array(all_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Train Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "  Train Loss: 0.087149, Train Acc: 0.9664\n",
            "  Val Loss: 0.082788, Val Acc: 0.9618\n",
            "\n",
            "Epoch 2/20\n",
            "  Train Loss: 0.080799, Train Acc: 0.9686\n",
            "  Val Loss: 0.081063, Val Acc: 0.9636\n",
            "\n",
            "Epoch 3/20\n",
            "  Train Loss: 0.076710, Train Acc: 0.9701\n",
            "  Val Loss: 0.078893, Val Acc: 0.9638\n",
            "\n",
            "Epoch 4/20\n",
            "  Train Loss: 0.074123, Train Acc: 0.9710\n",
            "  Val Loss: 0.080372, Val Acc: 0.9637\n",
            "\n",
            "Epoch 5/20\n",
            "  Train Loss: 0.071600, Train Acc: 0.9720\n",
            "  Val Loss: 0.070380, Val Acc: 0.9665\n",
            "\n",
            "Epoch 6/20\n",
            "  Train Loss: 0.070220, Train Acc: 0.9725\n",
            "  Val Loss: 0.074753, Val Acc: 0.9702\n",
            "\n",
            "Epoch 7/20\n",
            "  Train Loss: 0.068559, Train Acc: 0.9728\n",
            "  Val Loss: 0.070098, Val Acc: 0.9684\n",
            "\n",
            "Epoch 8/20\n",
            "  Train Loss: 0.066437, Train Acc: 0.9738\n",
            "  Val Loss: 0.076004, Val Acc: 0.9681\n",
            "\n",
            "Epoch 9/20\n",
            "  Train Loss: 0.066116, Train Acc: 0.9741\n",
            "  Val Loss: 0.070177, Val Acc: 0.9724\n",
            "\n",
            "Epoch 10/20\n",
            "  Train Loss: 0.067069, Train Acc: 0.9736\n",
            "  Val Loss: 0.075324, Val Acc: 0.9646\n",
            "\n",
            "Epoch 11/20\n",
            "  Train Loss: 0.066336, Train Acc: 0.9737\n",
            "  Val Loss: 0.070296, Val Acc: 0.9638\n",
            "\n",
            "Epoch 12/20\n",
            "  Train Loss: 0.060361, Train Acc: 0.9765\n",
            "  Val Loss: 0.070723, Val Acc: 0.9688\n",
            "Early stopping at epoch 12\n",
            "Loaded best model weights\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    print(f\"  Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.6f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    print()\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('best_transformer_model.pth'))\n",
        "print(\"Loaded best model weights\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss curves\n",
        "ax1.plot(train_losses, label='Train Loss')\n",
        "ax1.plot(val_losses, label='Val Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Accuracy curves\n",
        "ax2.plot(train_accs, label='Train Accuracy')\n",
        "ax2.plot(val_accs, label='Val Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11. Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss, test_acc, test_preds, test_labels = validate(model, test_loader, criterion, device)\n",
        "y_pred = (test_preds > 0.5).astype(int)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.6f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"\\nTest AUC-ROC: {roc_auc_score(y_test, test_preds):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 12. Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=['Benign', 'Attack'],\n",
        "            yticklabels=['Benign', 'Attack'])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Transformer IDS Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 13. ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test, test_preds)\n",
        "auc_score = roc_auc_score(y_test, test_preds)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'Transformer IDS (AUC = {auc_score:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Transformer IDS')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 14. Prediction Probability Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(test_preds[y_test == 0], bins=50, label=\"Benign\", stat=\"density\", alpha=0.7, kde=True)\n",
        "sns.histplot(test_preds[y_test == 1], bins=50, label=\"Attack\", stat=\"density\", alpha=0.7, kde=True)\n",
        "plt.axvline(x=0.5, color='r', linestyle='--', label='Decision Threshold (0.5)')\n",
        "plt.xlabel('Prediction Probability')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Prediction Probability Distribution (Test Set)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 15. Notes for Report\n",
        "\n",
        "- The transformer model is trained in a supervised manner on both benign and attack traffic.\n",
        "- Class imbalance is handled using weighted random sampling during training.\n",
        "- The model uses self-attention to learn relationships between network flow features.\n",
        "- Positional encoding is added to help the model understand feature positions.\n",
        "- Results show the transformer's ability to learn discriminative patterns for intrusion detection.\n",
        "- The model can be compared with the autoencoder approach to evaluate the benefits of supervised vs. unsupervised learning for IDS."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
